# tail.py (Phi√™n-b·∫£n-s·ª≠a-l·ªói-NoneType-v√†-hi·ªÉn-th·ªã)

import torch
import psutil
from ultralytics.nn.tasks import DetectionModel
import yaml
from ultralytics.utils.plotting import Annotator, colors
import cv2
from ultralytics.models.yolo.detect import DetectionPredictor
from ultralytics.utils import DEFAULT_CFG
import numpy as np


# ... (Ph·∫ßn 1, 2, 3, 4 gi·ªØ nguy√™n y h·ªát nh∆∞ phi√™n b·∫£n tr∆∞·ªõc) ...
def check_ram():
    return psutil.virtual_memory().used / (1024 ** 3)


ram_at_init = check_ram()
print("üöÄ Loading full architecture for tail...")
cfg = yaml.safe_load(open('yolo11n.yaml', 'r', encoding='utf-8'))
tail_model = DetectionModel(cfg, verbose=False)
ram_after_full_archi = check_ram()
print(f"[RAM usage after loading architecture: {ram_after_full_archi - ram_at_init:.4f} GB]")
print("   Loading state_dict for part2...")
state_dict_part2 = torch.load('part2.pt', map_location='cpu', weights_only=True)
tail_model.load_state_dict(state_dict_part2, strict=False)
tail_model.eval()
print("   ...Done.")
print("üó∫Ô∏è Loading feature map from 'feature_map.pt'...")
feature_map = torch.load('feature_map.pt', map_location='cpu', weights_only=True)
print(f"   Input feature map shape: {feature_map.shape}")


def forward_tail(model, feature_map_in):
    split_index = 4
    y = {}
    current_x = feature_map_in
    y[split_index - 1] = current_x
    for layer in model.model[split_index:]:
        if isinstance(layer.f, int):
            if layer.f == -1:
                x_in = current_x
            else:
                x_in = y[layer.f]
        else:
            x_in = []
            for from_index in layer.f:
                if from_index == -1:
                    x_in.append(current_x)
                else:
                    x_in.append(y[from_index])
        current_x = layer(x_in)
        y[layer.i] = current_x
    return current_x


print("üß† Performing custom forward pass on tail...")
with torch.no_grad():
    preds = forward_tail(tail_model, feature_map)
print("\n‚úÖ Inference done.")

# ================================================================= #
# ======== 5. H·∫≠u x·ª≠ l√Ω v√† v·∫Ω Bounding Box (ƒê√É S·ª¨A) ========
print("üîç Post-processing and drawing bounding boxes...")

# 5.1. T·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng predictor t√πy ch·ªânh
args = DEFAULT_CFG
# args.model = 'yolo11n.pt'
args.imgsz = 640

custom_predictor = DetectionPredictor(overrides=vars(args))
custom_predictor.model = tail_model

# 5.2. Load ·∫£nh g·ªëc v√† chu·∫©n b·ªã c√°c tham s·ªë c·∫ßn thi·∫øt
original_img_path = 'image.png'
img_to_draw = cv2.imread(original_img_path)
if img_to_draw is None:
    print(f"‚ùå Error: Could not read the original image at '{original_img_path}'")
    exit()

# ·∫¢nh g·ªëc c·∫ßn ƒë∆∞·ª£c chuy·ªÉn th√†nh m·ªôt list numpy array ƒë·ªÉ truy·ªÅn v√†o postprocess
orig_imgs = [img_to_draw]
# ·∫¢nh ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω (resize, to tensor, v.v.)
# Ch√∫ng ta s·∫Ω t·∫°o m·ªôt phi√™n b·∫£n gi·∫£ l·∫≠p
dummy_im = torch.zeros(1, 3, 640, 640)

# ‚úÖ S·ª¨A LOGIC ·ªû ƒê√ÇY: "Gi·∫£ l·∫≠p" thu·ªôc t√≠nh `batch` cho predictor
# self.batch c·∫ßn c√≥ c·∫•u tr√∫c: (paths, images, preprocessed_images, None)
custom_predictor.batch = [original_img_path], orig_imgs, dummy_im, None

# 5.3. G·ªçi h√†m postprocess t·ª´ predictor ƒë√£ t·∫°o
# preds l√† ƒë·∫ßu ra th√¥ t·ª´ m√¥ h√¨nh, orig_imgs l√† ·∫£nh g·ªëc ch∆∞a resize
results = custom_predictor.postprocess(preds, dummy_im, orig_imgs)
result = results[0]

# 5.4. V·∫Ω k·∫øt qu·∫£ l√™n ·∫£nh
boxes = result.boxes
if len(boxes) > 0:
    print(f"\n‚úÖ Found {len(boxes)} objects. Drawing them on the image...")

    # Kh√¥ng c·∫ßn load l·∫°i img_to_draw v√¨ ƒë√£ c√≥ orig_imgs[0]
    annotator = Annotator(orig_imgs[0], line_width=2, example=str(result.names))

    for box in boxes:
        class_id = int(box.cls)
        # T·ªça ƒë·ªô box.xyxy ƒë√£ ƒë∆∞·ª£c scale v·ªÅ k√≠ch th∆∞·ªõc ·∫£nh g·ªëc b·ªüi h√†m postprocess
        coords = box.xyxy[0].tolist()
        conf = float(box.conf)
        class_name = result.names[class_id]
        label = f'{class_name} {conf:.2f}'

        print(f"   - Object: {class_name}, Confidence: {conf:.2f}")
        annotator.box_label(coords, label, color=colors(class_id + 1, True))

    output_image = annotator.result()

else:
    print("\n‚úÖ No objects found.")
    output_image = orig_imgs[0]

# 5.5. Hi·ªÉn th·ªã ·∫£nh k·∫øt qu·∫£
print("\nüñºÔ∏è Displaying result image. Press any key to close.")
cv2.imshow("Detection Result", output_image)
cv2.waitKey(0)
cv2.destroyAllWindows()
# ================================================================= #